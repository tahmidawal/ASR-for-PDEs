# Better CNN Configuration: ~80-100K parameters
#
# Enhanced CNN with residual connections and larger receptive field
# Architecture: 5-layer CNN with [32, 64, 128, 64, 32] channels
# Features: GELU activation, residual connections every 2 layers
# Expected performance: 0.1-0.5% relative L2 error
#

train_dataset:
  dataset:
    name: poisson1d-synthetic
    args:
      num_samples: 10000
      resolution: 256
      freq_range: [1, 20]
      use_superposition: true
      num_components: 5
      use_float64: true
      normalize_solution: false
      seed: 42
  batch_size: 32

val_dataset:
  dataset:
    name: poisson1d-synthetic
    args:
      num_samples: 2000
      resolution: 256
      freq_range: [1, 20]
      use_superposition: true
      num_components: 5
      use_float64: true
      normalize_solution: true
      seed: 12345
  batch_size: 32

model:
  name: cnn-better
  args:
    channels: [32, 64, 128, 64, 32]
    kernel_sizes: [9, 7, 5, 7, 9]
    padding_mode: reflect
    activation: gelu
    use_residual: true

optimizer:
  name: adam
  args:
    lr: 1.0e-3
    betas: [0.9, 0.999]
    weight_decay: 1.0e-6

epoch_max: 1000
epoch_val: 1
early_stopping_patience: 75

reduce_lr_on_plateau:
  mode: min
  factor: 0.5
  patience: 20
  threshold: 1.0e-9
  min_lr: 1.0e-8
  cooldown: 5
  verbose: true

grad_clip: 1.0
use_double_precision: true
loss_type: l2  # Use L2 loss instead of MSE
